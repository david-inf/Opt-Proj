
@inproceedings{vaswani_painless_2019,
	title = {Painless stochastic gradient: Interpolation, line-search, and convergence rates},
	volume = {32},
	shorttitle = {Painless stochastic gradient},
	abstract = {Recent works have shown that stochastic gradient descent ({SGD}) achieves the fast convergence rates of full-batch gradient descent for over-parameterized models satisfying certain interpolation conditions. However, the step-size used in these works depends on unknown quantities and {SGD}'s practical performance heavily relies on the choice of this step-size. We propose to use line-search techniques to automatically set the step-size when training models that can interpolate the data. In the interpolation setting, we prove that {SGD} with a stochastic variant of the classic Armijo line-search attains the deterministic convergence rates for both convex and strongly-convex functions. Under additional assumptions, {SGD} with Armijo line-search is shown to achieve fast convergence for non-convex functions. Furthermore, we show that stochastic extra-gradient with a Lipschitz line-search attains linear convergence for an important class of non-convex functions and saddle-point problems satisfying interpolation. To improve the proposed methods' practical performance, we give heuristics to use larger step-sizes and acceleration. We compare the proposed algorithms against numerous optimization methods on standard classification tasks using both kernel methods and deep networks. The proposed methods result in competitive performance across all models and datasets, while being robust to the precise choices of hyper-parameters. For multi-class classification using deep networks, {SGD} with Armijo line-search results in both faster convergence and better generalization. Â© 2019 Neural information processing systems foundation. All rights reserved.},
	eventtitle = {Advances in Neural Information Processing Systems},
	author = {Vaswani, S. and Mishkin, A. and Laradji, I. and Schmidt, M. and Gidel, G. and Lacoste-Julien, S.},
	date = {2019},
	note = {{ISSN}: 1049-5258},
}

@inproceedings{fan_msl_2023,
	title = {{MSL}: An Adaptive Momentem-based Stochastic Line-search Framework},
	eventtitle = {{OPT} 2023: Optimization for Machine Learning},
	author = {Fan, Chen and Vaswani, Sharan and Thrampoulidis, Christos and Schmidt, Mark},
	date = {2023},
}
