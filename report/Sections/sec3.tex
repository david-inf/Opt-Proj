% !TeX spellcheck = en_GB
% ***************************************************** %
\section{Experiments and results discussion}\label{sc:exp}
% ***************************************************** %

%\begin{table}
%\caption[]{Hyper-parameters}
%\label{tab:hyper-params}
%\[
%\begin{array}{l}
%\beta=0.9 \\
%\end{array}
%\]
%\end{table}

%\begin{figure}
%\centering
%\includegraphics[width=0.3\textwidth]{../py/test.pdf}
%\end{figure}

% df.to_latex

To test the efficiency the algorithms, a benchmark of six datasets retrieved from \href{https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/}{LIBSVM} is used, see table~\vref{tab:datasets} for details. Every dataset comes already pre-processed, with every sample scaled in range $[-1,1]$; many features are categorical with values $0,1,2\dots$, this implies that the dataset matrix is sparse, so the \texttt{SciPy} CSR matrix format was used to store the data.

Compared to the available benchmark dataset, those chosen are not that large, the choice is due to the hardware available (Intel\textregistered\,Core\texttrademark\,i\num{7}). As can be seen few dataset are unbalanced, this will affect the accuracy.\par\smallskip

The regularization coefficient from~\eqref{eq:opt-prob} is set to $\lambda=0.5$ and the tolerance from the stopping criterion~\eqref{eq:stopping} $\epsilon=\num{e-3}$, then the momentum term $\beta_0=0.9$, the aggressiveness of the Armijo condition is set to a small value $\gamma=\num{e-3}$ and the maximum number of epochs is set to $k^\ast=600$. For the other hyper-parameters a \emph{grid search} is applied.

The grid search confronts different combinations of the mini-batch size, the learning rate in the basic SGD version and the ones with line search, in the latter are confronted also different values for the damping both in the Armijo method and momentum correction. The grid search for the mini-batch size depends on the considered dataset since those chosen are quite different in size, however the rule is to stay around the 100 iterations and the value goes with powers of 2, for the first five datasets the size starts from 32; follows the grid used for the others
\begin{center}
\begin{tabular}{lll}
\toprule
%Hyper-parameter & Algorithm & Values \\
%\midrule
$\alpha_0$ & \texttt{SGD-Fixed}, \texttt{SGDM} & \numlist{1; 0.5; 0.1; 0.01; 0.001; 0.0005; 0.0001; 0.00005} \\
$\alpha_0$ & \texttt{SGD-Decreasing} & \numlist{1; 0.8; 0.5; 0.1; 0.05; 0.01; 0.001; 0.0005} \\
$\alpha_0$ & \texttt{SGD-Armijo}, \texttt{MSL-SGDM-C/R} & \numlist{1; 0.1; 0.01; 0.005} \\
$\delta_a$ & \texttt{SGD-Armijo}, \texttt{MSL-SGDM-C/R} & \numlist{0.3; 0.5; 0.7; 0.9} \\
$\delta_m$ & \texttt{MSL-SGDM-C} & \numlist{0.5; 0.7} \\
\bottomrule
\end{tabular}
\end{center}
where $\delta_a$ is the damping for the Armijo line search and $\delta_m$ for the momentum correction, the combinations for the \texttt{MSL-SGDM-C} are twice the ones for \texttt{SGD-Armijo} and \texttt{MSL-SGDM-R}.

The grid search chooses the best solver based on the greater \emph{test accuracy} and lower \emph{objective function} value. The results can be seen in tables~\vref{tab:w1a-table} to~\ref{tab:german-tab}, the optimization problem is addressed also with the full-batch gradient descent and three solvers from \texttt{SciPy} which are \texttt{L-BFGS}, Conjugate Gradient and Newton-CG.\par\smallskip

Now as done by the authors of both articles, we want to show how the value of the objective function decreases with each epoch and time, we set $k^\ast=200$ and run the algorithms with the hyper-parameters from the grid search varying the learning rate for the grid \numlist{1; 0.1; 0.01}. Results can be seen in figures from~\vref{fig:w1a-w3a} to~\ref{fig:mush-german}.

% grid search
% i valori "ottimi" della grid search per i parametri che non sono il learning rate sono stati usati per mostrare poi l'andamento al variare del learning rate

\begin{table}
\centering
\caption{Benchmark datasets}
\label{tab:datasets}
\begin{tabular}{lSSSc}
\toprule
Name & {Train} & {Test} & {Features} & {Distribution} \\
\midrule
w1a & 2477 & 47272 & 300 & -1:$0.97$\,\,\,1:$0.03$ \\
w3a & 4912 & 44837 & 300 & -1:$0.97$\,\,\,1:$0.03$ \\
Phishing & 8844 & 2211 & 68 & -1:$0.45$\,\,\,1:$0.55$ \\
a2a & 2265 & 30296 & 119 & -1:$0.75$\,\,\,1:$0.25$ \\
Mushrooms & 6499 & 1625 & 112 & -1:$0.48$\,\,\,1:$0.52$ \\
German & 800 & 200 & 24 & -1:$0.70$\,\,\,1:$0.30$ \\
\bottomrule
\end{tabular}
\end{table}

%\cleardoublepage
\begin{table}
\sisetup{round-mode=places}
\centering
\caption{w1a dataset}
\label{tab:w1a-table}
\begin{tabular}{lS[round-precision=3]S[drop-zero-decimal]S[round-precision=4]S[round-precision=6]S[exponent-mode=scientific]S[round-precision=6]}
\toprule
Solver & {$\alpha_0$} & {Epochs} & {Run-time} & {$\func(w)$} & {$\nabla f(w)$} & {Test score} \\
\midrule
Newton-CG & NaN & 6 & NaN & 0.464614 & 0.000046 & 0.970236 \\
CG & NaN & 7 & NaN & 0.464614 & 0.000009 & 0.970236 \\
L-BFGS-B & NaN & 7 & NaN & 0.464614 & 0.000023 & 0.970236 \\
BatchGD-Fixed & 1.000000 & 12 & 0.010029 & 0.464614 & 0.000564 & 0.970236 \\
SGD-Decreasing & 0.500000 & 27 & 0.214942 & 0.464614 & 0.000792 & 0.970236 \\
SGD-Fixed & 0.010000 & 27 & 0.186510 & 0.464615 & 0.000852 & 0.970236 \\
SGDM & 0.010000 & 386 & 5.914988 & 0.464615 & 0.000978 & 0.970236 \\
MSL-SGDM-R & 0.005000 & 600 & 6.497748 & 0.464693 & 0.009144 & 0.970236 \\
MSL-SGDM-C & 0.005000 & 600 & 6.388432 & 0.464693 & 0.009149 & 0.970236 \\
SGD-Armijo & 0.100000 & 600 & 7.664835 & 0.536467 & 0.363829 & 0.971400 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}
\sisetup{round-mode=places}
\centering
\caption{w3a dataset}
\label{tab:w3a-tab}
\begin{tabular}{lS[round-precision=3]S[drop-zero-decimal]S[round-precision=4]S[round-precision=6]S[exponent-mode=scientific]S[round-precision=6]}
\toprule
Solver & {$\alpha_0$} & {Epochs} & {Run-time} & {$\func(w)$} & {$\nabla f(w)$} & {Test score} \\
\midrule
Newton-CG & NaN & 6 & NaN & 0.462742 & 0.000011 & 0.970203 \\
CG & NaN & 7 & NaN & 0.462742 & 0.000022 & 0.970203 \\
L-BFGS-B & NaN & 7 & NaN & 0.462742 & 0.000033 & 0.970203 \\
BatchGD-Fixed & 1.000000 & 12 & 0.016491 & 0.462742 & 0.000564 & 0.970203 \\
SGD-Decreasing & 0.500000 & 19 & 0.081762 & 0.462743 & 0.000876 & 0.970203 \\
SGD-Fixed & 0.010000 & 23 & 0.162157 & 0.462743 & 0.000949 & 0.970203 \\
SGDM & 0.100000 & 45 & 0.351333 & 0.462743 & 0.000895 & 0.970203 \\
MSL-SGDM-C & 0.005000 & 600 & 4.021617 & 0.462787 & 0.006924 & 0.970203 \\
MSL-SGDM-R & 0.005000 & 600 & 4.124630 & 0.462787 & 0.006924 & 0.970203 \\
SGD-Armijo & 0.010000 & 600 & 6.851229 & 0.500431 & 0.268456 & 0.971006 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}
\sisetup{round-mode=places}
\caption{Phishing dataset}
\label{tab:phish-tab}
\centering
\begin{tabular}{lS[round-precision=3]S[drop-zero-decimal]S[round-precision=4]S[round-precision=6]S[exponent-mode=scientific]S[round-precision=6]}
\toprule
Solver & {$\alpha_0$} & {Epochs} & {Run-time} & {$\func(w)$} & {$\nabla f(w)$} & {Test score} \\
\midrule
Newton-CG & NaN & 5 & NaN & 0.685065 & 0.000000 & 0.567616 \\
L-BFGS-B & NaN & 5 & NaN & 0.685065 & 0.000008 & 0.567616 \\
CG & NaN & 6 & NaN & 0.685065 & 0.000023 & 0.567616 \\
SGD-Decreasing & 0.100000 & 6 & 0.040321 & 0.685065 & 0.000508 & 0.567616 \\
SGDM & 0.100000 & 22 & 0.271079 & 0.685065 & 0.000575 & 0.567616 \\
BatchGD-Fixed & 1.000000 & 11 & 0.055085 & 0.685065 & 0.000534 & 0.567616 \\
SGD-Fixed & 0.010000 & 13 & 0.173696 & 0.685065 & 0.000927 & 0.567616 \\
MSL-SGDM-R & 0.100000 & 600 & 17.498211 & 0.685660 & 0.032621 & 0.568521 \\
MSL-SGDM-C & 1.000000 & 600 & 9.598539 & 0.685705 & 0.032668 & 0.568973 \\
SGD-Armijo & 0.005000 & 600 & 7.578606 & 0.687736 & 0.066541 & 0.865219 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}
\sisetup{round-mode=places}
\caption{a2a dataset}
\label{tab:a2a-tab}
\centering
\begin{tabular}{lS[round-precision=3]S[drop-zero-decimal]S[round-precision=4]S[round-precision=6]S[exponent-mode=scientific]S[round-precision=6]}
\toprule
Solver & {$\alpha_0$} & {Epochs} & {Run-time} & {$\func(w)$} & {$\nabla f(w)$} & {Test score} \\
\midrule
Newton-CG & NaN & 5 & NaN & 0.564027 & 0.000004 & 0.760265 \\
CG & NaN & 12 & NaN & 0.564027 & 0.000015 & 0.760265 \\
L-BFGS-B & NaN & 8 & NaN & 0.564027 & 0.000012 & 0.760265 \\
SGD-Decreasing & 0.800000 & 59 & 0.183221 & 0.564028 & 0.000726 & 0.760265 \\
SGDM & 0.100000 & 600 & 1.873068 & 0.564030 & 0.002628 & 0.760298 \\
MSL-SGDM-R & 0.010000 & 600 & 7.889470 & 0.577575 & 0.228297 & 0.790236 \\
MSL-SGDM-C & 0.010000 & 600 & 7.482898 & 0.579879 & 0.229332 & 0.789345 \\
BatchGD-Fixed & 1.000000 & 600 & 0.260017 & 0.594416 & 0.363976 & 0.822386 \\
SGD-Fixed & 1.000000 & 600 & 4.331557 & 0.602741 & 0.356183 & 0.807136 \\
SGD-Armijo & 1.000000 & 600 & 6.068792 & 0.617908 & 0.433035 & 0.798917 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}
\sisetup{round-mode=places}
\caption{Mushrooms dataset}
\label{tab:mush-tab}
\centering
\begin{tabular}{lS[round-precision=3]S[drop-zero-decimal]S[round-precision=4]S[round-precision=6]S[exponent-mode=scientific]S[round-precision=6]}
\toprule
Solver & {$\alpha_0$} & {Epochs} & {Run-time} & {$\func(w)$} & {$\nabla f(w)$} & {Test score} \\
\midrule
Newton-CG & NaN & 7 & NaN & 0.517726 & 0.000003 & 0.892923 \\
CG & NaN & 11 & NaN & 0.517726 & 0.000024 & 0.892923 \\
L-BFGS-B & NaN & 10 & NaN & 0.517726 & 0.000017 & 0.892923 \\
SGD-Decreasing & 0.100000 & 26 & 0.533000 & 0.517727 & 0.000779 & 0.893538 \\
BatchGD-Fixed & 0.500000 & 26 & 0.059922 & 0.517727 & 0.000757 & 0.892923 \\
SGD-Fixed & 0.500000 & 600 & 2.993838 & 0.525499 & 0.199874 & 0.926154 \\
MSL-SGDM-R & 0.100000 & 600 & 7.885503 & 0.527069 & 0.232121 & 0.940308 \\
MSL-SGDM-C & 0.100000 & 600 & 13.411705 & 0.527262 & 0.223900 & 0.939692 \\
SGD-Armijo & 0.100000 & 600 & 11.367419 & 0.535765 & 0.233549 & 0.953231 \\
SGDM & 1.000000 & 600 & 4.369383 & 0.557069 & 0.479065 & 0.924308 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}
\sisetup{round-mode=places}
\caption{German dataset}
\label{tab:german-tab}
\centering
\begin{tabular}{lS[round-precision=3]S[drop-zero-decimal]S[round-precision=4]S[round-precision=6]S[exponent-mode=scientific]S[round-precision=6]}
\toprule
Solver & {$\alpha_0$} & {Epochs} & {Run-time} & {$\func(w)$} & {$\nabla f(w)$} & {Test score} \\
\midrule
Newton-CG & NaN & 5 & NaN & 0.597303 & 0.000010 & 0.710000 \\
CG & NaN & 12 & NaN & 0.597303 & 0.000004 & 0.710000 \\
L-BFGS-B & NaN & 7 & NaN & 0.597303 & 0.000014 & 0.710000 \\
SGD-Fixed & 0.010000 & 58 & 0.129290 & 0.597303 & 0.000775 & 0.710000 \\
BatchGD-Fixed & 0.500000 & 20 & 0.011878 & 0.597303 & 0.000882 & 0.710000 \\
MSL-SGDM-R & 0.005000 & 600 & 2.221751 & 0.597456 & 0.023193 & 0.710000 \\
MSL-SGDM-C & 0.005000 & 600 & 2.861868 & 0.607466 & 0.140436 & 0.735000 \\
SGD-Decreasing & 0.010000 & 600 & 2.867500 & 0.607993 & 0.113583 & 0.720000 \\
SGD-Armijo & 0.100000 & 600 & 2.484220 & 0.614589 & 0.229813 & 0.740000 \\
SGDM & 1.000000 & 600 & 2.522513 & 0.616375 & 0.313714 & 0.745000 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}
\centering
% w1a
\subfloat[][\emph{w1a dataset}\label{subfig:w1a-diagnostic}]%
{\includegraphics[width=\textwidth]{w1a-diagnostic}} \\
% w3a
\subfloat[][\emph{w3a dataset}\label{subfig:w3a-diagnostic}]%
{\includegraphics[width=\textwidth]{w3a-diagnostic}} \\
\caption[]{w1a and w3a datasets}
\label{fig:w1a-w3a}
\end{figure}

\begin{figure}
\centering
% Phishing
\subfloat[][\emph{Phishing dataset}\label{subfig:phish-diagnostic}]%
{\includegraphics[width=\textwidth]{phish-diagnostic}} \\
% a2a
\subfloat[][\emph{a2a dataset}\label{subfig:a2a-diagnostic}]%
{\includegraphics[width=\textwidth]{a2a-diagnostic}}
\caption[]{Phishing and a2a datasets}
\label{fig:phish-a2a}
\end{figure}

\begin{figure}
\centering
% mushrooms
\subfloat[][\emph{Mushrooms dataset}\label{subfig:mush-diagnostic}]%
{\includegraphics[width=\textwidth]{mush-diagnostic}} \\
% german
\subfloat[][\emph{German dataset}\label{subfig:german-diagnostic}]%
{\includegraphics[width=\textwidth]{german-diagnostic}}
\caption[]{Mushrooms and German datasets}
\label{fig:mush-german}
\end{figure}

%\cleardoublepage
% ***************************************************** %
%\section{Mathematical background}
% ***************************************************** %

%\begin{defs}[Convex function]\label{def:conv_fun}
%	Let $S\subseteq\R^n$ be a convex set, a function $f\colon S\to\R$ is said to be convex if the hessian matrix is semi-positive-defined. If the hessian matrix is positive-defined, then the function is strictly convex.
%\end{defs}
%
%\begin{thm}[Weirstrass theorem]\label{thm:weirs}
%	Let $f\colon\R^n\to\R$ be a continuous function and $S\subseteq\R^n$ a compact set. Then function $f$ admits global minimum in $S$.
%\end{thm}
%
%\begin{cor}[Sufficient condition]\label{cor:weirs1}
%	If function $f\colon\R^n\to\R$ is a continuous and coercive function, then $f$ admits global minimum in $\R^n$.
%\end{cor}
%
%\begin{prop}[Coercivity of a quadratic function]
%	A quadratic function $\func(x)=\frac{1}{2}x^TQx-c^Tx$ is said to be coercive if and only if the symmetric matrix $Q\in\R^{n\times n}$ is positive-defined.
%\end{prop}
%
%\begin{prop}[Unique global minimum]\label{prop:min_unique}
%	Let $S\subseteq\R^n$ be a convex set, let $f\colon S\to\R$ be a strictly convex function. Then the global minimum, if exists, is unique.
%\end{prop}
%
%\begin{prop}[First order optimality condition]
%	$\bar{x}$ is a local minimum for $f\colon\R^n\to\R$ of class $f\in C^1(\R^n)$ if and only if $\nabla\func(\bar{x})=0$.
%\end{prop}
%
%\begin{prop}[Second order optimality condition]\label{prop:opt_second}
%	$\bar{x}\in\R^n$ is a local minimum for $f\colon\R^n\to\R$ of class $f\in C^2(\R^n)$ if and only if
%	\[\nabla\func(\bar{x})=0\quad\wedge\quad \nabla^2\func(\bar{x})\,\,\,\text{positive semi-definite}\]
%\end{prop}

% add definition of coercive function and proposition about compact sets?
% add gradient descent?
